WEBVTT

1
00:01:29.500 --> 00:01:35.839
stein.aerts@vib.be: Hello, people on Zoom. We will start a little later, because we're in a different room, and I need to wait for people to filter over here. So

2
00:01:37.270 --> 00:01:39.459
stein.aerts@vib.be: inevitably it takes people some time.

3
00:01:46.820 --> 00:01:48.890
stein.aerts@vib.be: We'll only give it 5 min or so.

4
00:06:56.240 --> 00:07:02.700
stein.aerts@vib.be: There we go. Zoom can hear us as well now. Yeah. So we're gonna get started here. Sorry for the delay.

5
00:07:04.000 --> 00:07:04.890
stein.aerts@vib.be: but

6
00:07:06.000 --> 00:07:15.311
stein.aerts@vib.be: we have 3 speakers today 2 of them are remote after Gabrielle, who is presenting first? st and I'm just gonna hand straight over. So

7
00:07:15.750 --> 00:07:20.020
stein.aerts@vib.be: 1st speaker is Gabrielle, who's going to talk to us about some Nanobody design?

8
00:07:21.510 --> 00:07:22.390
stein.aerts@vib.be: So

9
00:07:22.740 --> 00:07:23.540
stein.aerts@vib.be: Hi.

10
00:07:24.830 --> 00:07:41.079
stein.aerts@vib.be: so my name is Gabriela. I'm a postdoc at switch lab. So today I'm going to talk about basically my main postdoc project that is, now I'm finishing my postdoc. So I decided to wrap everything up in a single presentation. And that's what I'm gonna

11
00:07:41.310 --> 00:07:50.989
stein.aerts@vib.be: present. So my main topic was to design nanobodies for a specific target. Nanobodies are like antibodies, but with a single chain.

12
00:07:51.100 --> 00:07:52.310
stein.aerts@vib.be: So let's go through it.

13
00:07:52.320 --> 00:07:55.070
stein.aerts@vib.be: So this is an antibody.

14
00:07:55.765 --> 00:08:03.200
stein.aerts@vib.be: Antibody and nanobodies are, as I was saying, very similar they are. The nanobodies are

15
00:08:03.380 --> 00:08:12.220
stein.aerts@vib.be: are only this part, so only the heavy chain of an antibody. They are kind of different from a structural point of view, but conceptually they are very similar.

16
00:08:12.270 --> 00:08:26.820
stein.aerts@vib.be: So basically, I got asked to design nanobodies. But I was basically dealing with both antibodies and nanobodies. So whenever I talk about antibodies representation, I'm also going to include and nanobodies by definition

17
00:08:27.220 --> 00:08:41.399
stein.aerts@vib.be: anyway. So let's 1st talk about antibodies and antibodies, so what are antibodies? They are a protein complex that is in charge of recognizing external particle inside the body.

18
00:08:41.460 --> 00:08:55.709
stein.aerts@vib.be: So how it is. This done, it's done via a region that is called the Cdr complementary determining region. So this part is a part of a protein that is made of 3 loops

19
00:08:56.010 --> 00:09:11.700
stein.aerts@vib.be: that are hyperviable. What does that mean? It means that the sequence in that portion is extremely different from an antibody to another, and that means that it can be adapted in order to recognize different antigens.

20
00:09:11.730 --> 00:09:16.052
stein.aerts@vib.be: So since this part is hyper variable?

21
00:09:17.000 --> 00:09:23.309
stein.aerts@vib.be: yeah, it can be adapted. But it also means that it's very difficult to model from the computational point of view.

22
00:09:23.680 --> 00:09:38.760
stein.aerts@vib.be: So this is kind of a Holy Grail, for of the recent years in medical science, because if you are capable of designing antibodies. Computationally, you basically can apply them everywhere in medicine.

23
00:09:39.270 --> 00:09:40.345
stein.aerts@vib.be: So

24
00:09:41.620 --> 00:09:49.999
stein.aerts@vib.be: the problem is that we don't have enough data to predict the structure of this region that is in charge of recognizing the Antigen.

25
00:09:50.210 --> 00:10:02.009
stein.aerts@vib.be: even Alphafold. All the versions, even if they claim like better performances of the last version, fail in order in predicting the structure of this part.

26
00:10:02.250 --> 00:10:03.769
stein.aerts@vib.be: So what I got asked

27
00:10:04.450 --> 00:10:10.860
stein.aerts@vib.be: is to to make a tool to do, to predict it. And 1st of all, I screened the state of art.

28
00:10:11.590 --> 00:10:28.479
stein.aerts@vib.be: Okay, so basically, what everybody does is splitting the problem in 2 parts. The 1st one is predicting the structure of a Cdr. So this hypervariable region, how is going to organize itself in three-dimensional space?

29
00:10:28.650 --> 00:10:35.319
stein.aerts@vib.be: And then they try to fit the antigen on the predictive structure with a process called docking.

30
00:10:35.600 --> 00:10:50.430
stein.aerts@vib.be: So these 2 processes are basically handled by 2 different tools, usually, but also by 2 different approaches. One is machine learning, based usually, and the other one is

31
00:10:51.340 --> 00:10:53.969
stein.aerts@vib.be: physical, based. So true force fields.

32
00:10:55.080 --> 00:11:08.889
stein.aerts@vib.be: Only recently there has been some some models that anyway, they do not work that try to model the ensemble altogether that are based on the same approach as alphafoso on transformers.

33
00:11:11.180 --> 00:11:12.030
stein.aerts@vib.be: So

34
00:11:12.410 --> 00:11:16.839
stein.aerts@vib.be: what I wanted to do is exactly the same, but in an end-to-end way.

35
00:11:16.870 --> 00:11:21.789
stein.aerts@vib.be: So what I wanted to do is predicting the structure.

36
00:11:22.030 --> 00:11:26.749
stein.aerts@vib.be: the doking, and also the affinity of antibody in a single go.

37
00:11:27.470 --> 00:11:35.191
stein.aerts@vib.be: So 1st of all, what's and what end to end mean? I think that most of you probably already know it. But just to

38
00:11:35.800 --> 00:11:36.989
stein.aerts@vib.be: as a reminder.

39
00:11:37.040 --> 00:11:48.929
stein.aerts@vib.be: So let's say that we have a neuron. We have a problem in which we have some inputs and some outputs. In this case, we want to predict which are the peaks and which are one, are not

40
00:11:49.090 --> 00:11:54.990
stein.aerts@vib.be: so. We can have a neural network with initially random parameters.

41
00:11:55.220 --> 00:11:57.599
stein.aerts@vib.be: and we get some random predictions.

42
00:11:58.230 --> 00:12:10.069
stein.aerts@vib.be: The goal of a neural network is to find the way in which we can change the weights in order to maximize the expected and the actual output of the network.

43
00:12:11.420 --> 00:12:16.349
stein.aerts@vib.be: In order to do so, we calculate the gradient of the weights of a network with respect

44
00:12:16.400 --> 00:12:19.909
stein.aerts@vib.be: to to a score that is called loss function.

45
00:12:20.490 --> 00:12:28.410
stein.aerts@vib.be: And then we do some grad in the sand. So basically, we push the weights in a direction in which, iteratively.

46
00:12:28.480 --> 00:12:34.210
stein.aerts@vib.be: we can better the difference between the expected and the actual out of net.

47
00:12:36.210 --> 00:12:46.560
stein.aerts@vib.be: So in order to calculate a gradient, everything, every step inside the network needs to be differentiable, because otherwise it doesn't make any sense from a mathematical point of view.

48
00:12:47.050 --> 00:12:56.810
stein.aerts@vib.be: and a pipeline that is composed of only differentiable steps, in which you make a single gradient calculation in one go is called end to end.

49
00:12:57.160 --> 00:12:58.059
stein.aerts@vib.be: And that's it.

50
00:12:59.020 --> 00:13:04.219
stein.aerts@vib.be: So what I wanted to do basically is to have a set of

51
00:13:04.260 --> 00:13:06.879
stein.aerts@vib.be: sequences defining the Cdr

52
00:13:07.190 --> 00:13:11.330
stein.aerts@vib.be: using a neural network to calculate the confirmation.

53
00:13:11.430 --> 00:13:15.660
stein.aerts@vib.be: another neural network, to calculate the docking together with affinity.

54
00:13:16.070 --> 00:13:18.140
stein.aerts@vib.be: evaluable loss.

55
00:13:18.210 --> 00:13:24.679
stein.aerts@vib.be: and make a gradient calculation, and an update of the weights of a network in a single go.

56
00:13:27.240 --> 00:13:41.729
stein.aerts@vib.be: So, of course, in this, every step of this pipeline needs to be differentiable, otherwise it wouldn't be end to end. By the way, again, probably you already know it. But by now, basically, every modern machine learning method is an end to end.

57
00:13:43.570 --> 00:13:49.139
stein.aerts@vib.be: So this is the network that I initially developed like a very simplified version.

58
00:13:49.980 --> 00:14:00.409
stein.aerts@vib.be: And as loss function, I was using the Rmsd, that is a measure of a difference between the actual and the predictive structure of an antibody or nanobody.

59
00:14:01.671 --> 00:14:09.379
stein.aerts@vib.be: So I initially faced some problem in developing the architecture of a docking part, because I really didn't know how to deal with that.

60
00:14:09.800 --> 00:14:21.859
stein.aerts@vib.be: So at a certain point, I thought, Okay, what what happens when I have to deal with with an object in in outside biology. So let's say that we have an object, a rabbit.

61
00:14:22.130 --> 00:14:29.430
stein.aerts@vib.be: What I do in order to recognize a rabbit with machine learning is to convert it to some data structures.

62
00:14:29.700 --> 00:14:30.530
stein.aerts@vib.be: Then

63
00:14:30.710 --> 00:14:35.640
stein.aerts@vib.be: I have is architecture that is specific or specific data structure.

64
00:14:35.750 --> 00:14:37.950
stein.aerts@vib.be: And I get the results.

65
00:14:38.270 --> 00:14:54.299
stein.aerts@vib.be: So what if I have my protein, and I convert it to the same data structure, so that I don't have to develop an architecture by myself. I can use a 1 that has been used for being developed to deal with other objects that are not biologically related.

66
00:14:55.440 --> 00:15:07.709
stein.aerts@vib.be: So basically, if I can do that, I just can avoid doing that. And every bioinformatician is so lazy that if it can avoid doing work it will definitely.

67
00:15:08.410 --> 00:15:09.260
stein.aerts@vib.be: So

68
00:15:09.850 --> 00:15:16.239
stein.aerts@vib.be: basically, if I have my antibody, can, I convert it to something that can be used like, yeah, something else.

69
00:15:17.040 --> 00:15:23.529
stein.aerts@vib.be: Unfortunately, there was nothing that would allow me to do it in a end-to-end way.

70
00:15:23.830 --> 00:15:24.660
stein.aerts@vib.be: and

71
00:15:25.630 --> 00:15:36.090
stein.aerts@vib.be: in this way, not doing it end to end. The architecture would have the my network would have been blocked from a back propagation point of view, and the model wouldn't have worked.

72
00:15:36.150 --> 00:15:40.740
stein.aerts@vib.be: so I had to develop it, and it had been published 2 years ago already.

73
00:15:42.520 --> 00:15:53.840
stein.aerts@vib.be: anyway, this is a library that is, as a general purpose. And now I'm going to show you a video of like, yeah, another usage that it can be used apart from antibodies

74
00:15:53.870 --> 00:15:59.968
stein.aerts@vib.be: as a general purpose, because basically, I try to always develop tool that have the largest

75
00:16:00.640 --> 00:16:02.070
stein.aerts@vib.be: possible application.

76
00:16:02.190 --> 00:16:09.390
stein.aerts@vib.be: And, for instance, it can be used in order to perform machine learning based doking of small molecules and proteins.

77
00:16:11.550 --> 00:16:13.180
stein.aerts@vib.be: So this is where

78
00:16:13.740 --> 00:16:16.630
stein.aerts@vib.be: repositioning or left, you said right

79
00:16:17.670 --> 00:16:25.450
stein.aerts@vib.be: repositioning of the of a small molecule inside the structure. This is done entirely from a machine learning neural network

80
00:16:25.540 --> 00:16:39.410
stein.aerts@vib.be: without the help of any physical model. The red shade was the actual position of a small molecule, and the molecule was initially, randomly put in the pocket and was

81
00:16:39.810 --> 00:16:40.760
stein.aerts@vib.be: put back

82
00:16:40.860 --> 00:16:42.320
stein.aerts@vib.be: by the neural network.

83
00:16:43.540 --> 00:16:48.900
stein.aerts@vib.be: So yeah, so back to my topic. So I had the library to convert a protein.

84
00:16:50.240 --> 00:16:51.979
stein.aerts@vib.be: or I'm checking the time

85
00:16:52.120 --> 00:16:59.960
stein.aerts@vib.be: to convert the the antibody in a suitable data structure at the architecture that I got from.

86
00:17:00.540 --> 00:17:04.159
stein.aerts@vib.be: and image recognition from architecture, by the way.

87
00:17:05.220 --> 00:17:17.950
stein.aerts@vib.be: And so I had the full pipeline. But when I started testing it, like I was getting some good results. These are the loops that some free loops of a Cdr. That I predicted they're quite similar.

88
00:17:18.140 --> 00:17:20.829
stein.aerts@vib.be: The point is that on like.

89
00:17:21.700 --> 00:17:25.349
stein.aerts@vib.be: together with these, I was also getting this type of stuff

90
00:17:25.450 --> 00:17:26.910
stein.aerts@vib.be: that are like.

91
00:17:27.380 --> 00:17:40.780
stein.aerts@vib.be: The these are what is called intern is called clashes. So this is something that is physically impossible because 2 atoms cannot be so close. And the network was clearly learning a bias. Here.

92
00:17:40.830 --> 00:17:52.649
stein.aerts@vib.be: The problem is that the loss function was not that bad because the Rmsd was about 4. So it's still acceptable. But this solution shouldn't be explored by network, because it's by definition wrong.

93
00:17:52.880 --> 00:17:54.600
stein.aerts@vib.be: So what I said is.

94
00:17:55.070 --> 00:17:57.479
stein.aerts@vib.be: can I avoid the network

95
00:17:57.680 --> 00:18:02.740
stein.aerts@vib.be: looking into this structure, just putting a manifold inside the

96
00:18:02.940 --> 00:18:18.590
stein.aerts@vib.be: inside the network. So a manifold is like, imagine having a three-dimensional space. If you have a sphere inside the the 3 dimensional space, and you know that the solution is a surface, your your space become 2 dimensional.

97
00:18:19.457 --> 00:18:25.660
stein.aerts@vib.be: This reduces the parameters of an of of like the number of dimension by like heavily

98
00:18:25.820 --> 00:18:31.739
stein.aerts@vib.be: in a multi dimensional space. And that's what I wanted to do in the network that I was developing.

99
00:18:32.150 --> 00:18:32.920
stein.aerts@vib.be: So

100
00:18:34.070 --> 00:18:48.540
stein.aerts@vib.be: what does this mean? Conformational energy? It means that you need to implement a force field. So a set of physical rules in a way that a network can calculate the grading through.

101
00:18:49.100 --> 00:19:02.920
stein.aerts@vib.be: Of course this thing doesn't exist, so I had to develop myself. Fortunately, in my lab. There was this force field called Foldex. That is very well known. And I had the code off. So I said, Okay, let's just convert in Python.

102
00:19:04.210 --> 00:19:07.549
stein.aerts@vib.be: This happened in in 2019.

103
00:19:08.190 --> 00:19:10.150
stein.aerts@vib.be: This is what happened after

104
00:19:10.730 --> 00:19:28.190
stein.aerts@vib.be: there was a lot of crying involved. It took me like 4 years. It was 500,000 lines of code in C, written in 2,001 by from biologists in single core, without anything related to Gpus or prioritization.

105
00:19:28.290 --> 00:19:50.329
stein.aerts@vib.be: It was a mess, but at the end of the day I managed to develop the tool that basically, it's a force field. But it's also a neural network layer. So from the computational point of view, it doesn't change like there is absolutely no different of a neural network layer without learning parameters, so you can plug it wherever you want

106
00:19:50.580 --> 00:19:53.949
stein.aerts@vib.be: again. This is a

107
00:19:54.080 --> 00:20:04.230
stein.aerts@vib.be: a general purpose tool. I wanted to use it for nanobodies, but this can be used to everything, and like any linear layer in your your

108
00:20:04.570 --> 00:20:12.360
stein.aerts@vib.be: neural network. So it's a net is a layer that takes a set of coordinates as input and provides an energies output. And that's it.

109
00:20:12.810 --> 00:20:32.089
stein.aerts@vib.be: Yeah, it has been recently published, together with a patent. I'm just going to show you how you can make different stuff the forces in different positions in a pipeline. So a typical problem in bioinformatics is that

110
00:20:32.270 --> 00:20:34.830
stein.aerts@vib.be: and structural relaxation. So when you have

111
00:20:34.850 --> 00:20:50.700
stein.aerts@vib.be: a protein structure that has been experimentally solved, the position of the atoms is not is not like. There is some, some experimental error inside. So you often want to minimize the energy in order to have a more coherent structure.

112
00:20:51.147 --> 00:21:02.230
stein.aerts@vib.be: This is done from by forces, you know, in like finding the minimum of energy of a structure moving a little of the atoms so that they fit more in like

113
00:21:02.380 --> 00:21:04.970
stein.aerts@vib.be: they are more physically tuned.

114
00:21:07.200 --> 00:21:08.600
stein.aerts@vib.be: So yeah.

115
00:21:08.650 --> 00:21:12.090
stein.aerts@vib.be: you have some experimental error. You move the atoms.

116
00:21:12.550 --> 00:21:13.340
stein.aerts@vib.be: So

117
00:21:13.710 --> 00:21:32.539
stein.aerts@vib.be: in 4 lines of code. You can make a structural relaxation tool using the force field. Because, yeah, you can calculate the gradient out of this. So if you calculate the gradient. With respect to the position of the atoms. What you get is find the position of the atom that minimize the energy.

118
00:21:32.930 --> 00:21:38.519
stein.aerts@vib.be: And I did it for a small proteins, because protein, because, yeah, it takes quite a lot of time.

119
00:21:38.540 --> 00:21:48.550
stein.aerts@vib.be: So it's I shuffled the protein, and I asked the the pipeline to put it back to its original, to the minimum of energy. You can see that

120
00:21:48.800 --> 00:21:55.170
stein.aerts@vib.be: the secondary structure are initially absent, and then they got form with time.

121
00:21:57.410 --> 00:22:03.330
stein.aerts@vib.be: But this is just one of examples. You can also, for instance, implement

122
00:22:04.000 --> 00:22:06.040
stein.aerts@vib.be: implement motions.

123
00:22:06.050 --> 00:22:20.949
stein.aerts@vib.be: So here I implement motion like giving an initial speed to the atoms, and then seeing how the system was evolving. So basically molecular dynamics, this is with a small initial speed. So basically low temperature.

124
00:22:21.140 --> 00:22:22.260
stein.aerts@vib.be: same protein.

125
00:22:28.110 --> 00:22:28.900
stein.aerts@vib.be: Hmm.

126
00:22:29.260 --> 00:22:29.980
stein.aerts@vib.be: okay.

127
00:22:30.630 --> 00:22:32.529
stein.aerts@vib.be: And this is with high temperature.

128
00:22:32.750 --> 00:22:34.250
stein.aerts@vib.be: So it's unfolding.

129
00:22:36.530 --> 00:22:37.275
stein.aerts@vib.be: So

130
00:22:38.290 --> 00:22:46.229
stein.aerts@vib.be: basically, what I did is including other like this stuff inside my pipeline. And what I got was like a full pipeline.

131
00:22:46.650 --> 00:22:54.519
stein.aerts@vib.be: And now I'm going to show you the very last thing of my project. That is basically the experimental validation.

132
00:22:54.920 --> 00:23:05.979
stein.aerts@vib.be: So a couple of things about my philosophy. I don't trust anything, especially in bioinformatics, because biological data sets are always full of biases and networks, love biases.

133
00:23:06.200 --> 00:23:14.269
stein.aerts@vib.be: So whenever I can, I do experimental validation. I don't trust anything that has never been experimentally validated.

134
00:23:15.560 --> 00:23:22.700
stein.aerts@vib.be: So I got asked to design an antibody a nanobody this time, because nanobodies are cheaper than antibodies.

135
00:23:23.486 --> 00:23:24.143
stein.aerts@vib.be: To

136
00:23:25.400 --> 00:23:31.200
stein.aerts@vib.be: bind the il 9 receptor that is a receptor for interleukin 9.

137
00:23:32.150 --> 00:23:45.369
stein.aerts@vib.be: So the goal was to bind here. So in this part. This is the ligand interleukin. This part was not going to interfere with the binding of the interleukin. That was what we wanted to achieve.

138
00:23:45.820 --> 00:23:55.250
stein.aerts@vib.be: So what I did is I used my pipeline to design a library of nanobodies. I designed 1,800, I think.

139
00:23:57.180 --> 00:24:06.219
stein.aerts@vib.be: I done the these 2 guys that are 2 of my colleagues made what's called a page display. So they expressed

140
00:24:06.350 --> 00:24:07.090
stein.aerts@vib.be: the

141
00:24:08.260 --> 00:24:09.969
stein.aerts@vib.be: Nanobody in a

142
00:24:11.120 --> 00:24:13.580
stein.aerts@vib.be: pages than the the

143
00:24:13.650 --> 00:24:19.770
stein.aerts@vib.be: Nanobody was attached to a to a probe, and then the binding was

144
00:24:19.970 --> 00:24:20.800
stein.aerts@vib.be: tested.

145
00:24:21.560 --> 00:24:34.299
stein.aerts@vib.be: So the experiments are still ongoing. But what we know right now is that we managed to find 16 yeah, 16 hits that was binding. We're binding the

146
00:24:34.400 --> 00:24:35.650
stein.aerts@vib.be: the target.

147
00:24:35.670 --> 00:24:44.750
stein.aerts@vib.be: And one of these at 39 nanomora of affinity that is extremely high, even for market-based nanobodies.

148
00:24:44.770 --> 00:24:54.969
stein.aerts@vib.be: Also, it's not competing with the Antigen. That means that there, it's binding in a different place. With respect to the Antigen. That's what we wanted.

149
00:24:55.190 --> 00:25:04.190
stein.aerts@vib.be: Now, what we are we are waiting for is a stability, some more precise calculation of affinity, and the the X-ray structure.

150
00:25:05.350 --> 00:25:06.400
stein.aerts@vib.be: And that was it.

151
00:25:10.890 --> 00:25:18.000
stein.aerts@vib.be: You can use this one. Thank you very much, Gabrielle. Do we have any questions in the audience or on Zoom?

152
00:25:18.500 --> 00:25:22.090
stein.aerts@vib.be: If you are on zoom, you can either type in the chat or speak up.

153
00:25:23.066 --> 00:25:25.090
stein.aerts@vib.be: While we yeah. Test.

154
00:25:30.940 --> 00:25:38.550
stein.aerts@vib.be: Yes, thank you. Thank you. Really cool talk, and seems like it must have been a lot of work, especially all the implementations.

155
00:25:39.084 --> 00:25:49.750
stein.aerts@vib.be: I was wondering, do you have anything specific in mind to go? As you said, you were trying to make this very general, like any specific applications beyond the antibody Nanobody

156
00:25:49.810 --> 00:25:56.920
stein.aerts@vib.be: sphere. Because, of course, they are like they're you're just optimizing the highly variable region and

157
00:25:57.140 --> 00:25:59.279
stein.aerts@vib.be: everyone else. Not that much. Do you see.

158
00:25:59.680 --> 00:26:06.389
stein.aerts@vib.be: do you? Where do you see this going next? Do you think this is more broadly applicable in other fields of, let's say, protein design.

159
00:26:07.700 --> 00:26:13.289
stein.aerts@vib.be: So right now, I can tell you what I'm want, I want to do with that. So

160
00:26:14.650 --> 00:26:32.279
stein.aerts@vib.be: the all the parts here are basically independent. So I'm nowadays optimizing the hyper variable region, as you were saying. But yeah, the same pipeline changing couple of parser can be used in order to optimize whatever of course it needs to be. Have some logic behind.

161
00:26:32.410 --> 00:26:41.069
stein.aerts@vib.be: I'm now thinking about how to develop an architecture, to deal with disorder proteins, and I want to to use the force field.

162
00:26:41.090 --> 00:26:51.499
stein.aerts@vib.be: So not the the library. The 1st library I was talking about, but the force, in order to model the transient interaction between

163
00:26:51.690 --> 00:26:53.990
stein.aerts@vib.be: disordered proteins. And

164
00:26:54.420 --> 00:26:55.790
stein.aerts@vib.be: this is

165
00:26:56.130 --> 00:27:19.729
stein.aerts@vib.be: this is going to be tricky, but it's also something that I really think that needs to be done in order to have something that works in disorder proteins. Because we don't have data. And when you don't have data, you have 2 options. One is reducing the number of parameters of a network, reducing the capability of learning. And one is making manifolds inside the network.

166
00:27:19.820 --> 00:27:30.040
stein.aerts@vib.be: And I'm going to go for a last. And I'm going to try to model this transient interaction, maximizing them during the

167
00:27:30.330 --> 00:27:40.420
stein.aerts@vib.be: in the architecture is, gonna be tricky, because most probably it's gonna require some higher order optimization inside the network. And I'm not sure it's gonna work.

168
00:27:40.700 --> 00:27:41.580
stein.aerts@vib.be: Oh, let's see.

169
00:27:41.800 --> 00:27:43.419
stein.aerts@vib.be: Very cool, very cool. Thank you.

170
00:27:44.470 --> 00:27:48.579
stein.aerts@vib.be: Okay. And I will ask one quick one. So you

171
00:27:48.600 --> 00:27:55.490
stein.aerts@vib.be: predicted 1,800 or over a thousand nanobodies, and you know you had a small selection of about 16 that were

172
00:27:55.610 --> 00:28:09.389
stein.aerts@vib.be: had a quite decent affinity. Is there any parameters you can use within your network to to narrow your search space before experimentally validating like? Was there anything about those 16 that stood out in terms of

173
00:28:09.420 --> 00:28:12.739
stein.aerts@vib.be: anything that you can. So from 16, like.

174
00:28:13.200 --> 00:28:15.870
stein.aerts@vib.be: if you're thinking, like, yeah, these 60 worked.

175
00:28:16.360 --> 00:28:25.470
stein.aerts@vib.be: how can what we can learn very little, because, yeah, unfortunately, the space of a of a sequence is so large that 16 is not a statistical sum.

176
00:28:25.610 --> 00:28:32.229
stein.aerts@vib.be: Also this, like overthinking about your hits might lead to 2 things. So

177
00:28:32.510 --> 00:28:45.150
stein.aerts@vib.be: 1st of all, if you try to learn from the very small sample you might like end up in a local minima. Sure. So that's dangerous. On the other side.

178
00:28:45.340 --> 00:28:56.680
stein.aerts@vib.be: you might end up having, like, overthink, like ordering new nanobodies based on that specific sample and then realize that your some, your, your previous experiments.

179
00:28:56.930 --> 00:28:59.579
stein.aerts@vib.be: had some problems. So yeah, yeah.

180
00:28:59.860 --> 00:29:17.609
stein.aerts@vib.be: it's always good to stay as general as possible. In my opinion, what I'm now doing is trying not to learn from them, but to learn from the patented antibodies that got available, so that I know that they work and they work till the market. So

181
00:29:18.730 --> 00:29:36.790
stein.aerts@vib.be: I'm doing a language model to make some constraint of a sequence that can be explored. So it's another way of doing a manicode, indeed. But that's not done in an end-to-end way. Yeah, okay, that's cool. Okay, with that, we're going to move on to our second speaker. So thank you again, Gabrielle.

182
00:29:39.870 --> 00:29:47.880
stein.aerts@vib.be: Okay, our second speaker today is wim kuipers, and I see, is ready online already, and

183
00:29:48.370 --> 00:29:52.020
stein.aerts@vib.be: so I will stop sharing, and you should be able to take over when.

184
00:29:53.550 --> 00:29:54.600
wim cuypers: Okay.

185
00:30:00.470 --> 00:30:04.370
wim cuypers: okay, while I'm setting up. Can you maybe confirm that you can hear me loud and clear.

186
00:30:04.370 --> 00:30:06.819
stein.aerts@vib.be: Can hear you. Yeah, perfect. And.

187
00:30:06.820 --> 00:30:07.770
wim cuypers: Date, once.

188
00:30:08.540 --> 00:30:09.460
stein.aerts@vib.be: The.

189
00:30:10.930 --> 00:30:11.550
wim cuypers: Okay.

190
00:30:11.550 --> 00:30:15.169
stein.aerts@vib.be: Share is, yeah, we see the share. And we see exactly what we need to see.

191
00:30:15.450 --> 00:30:16.800
wim cuypers: Perfect. Thank you very much.

192
00:30:16.990 --> 00:30:25.970
wim cuypers: Okay. My name is Rim. I'm a postdoc at the level of Professor Peace Logins in the Atram Data lab and at University of Antwerp.

193
00:30:26.160 --> 00:30:42.749
wim cuypers: I will tell you about my journey from Phd. Student to Postdoc, where I went from studying Salmonella to now studying Nanopore squirrels, and I decided to show multiple projects and zoom out a little bit and talk about the flow between these projects as well.

194
00:30:43.060 --> 00:30:48.828
wim cuypers: I'll 1st start with explaining why I'm I'm I study infectious diseases.

195
00:30:49.750 --> 00:30:59.080
wim cuypers: why or why? It's important to study infectious diseases, because it's still a threat even nowadays. And nowadays the threat comes from zoonotic diseases vector borne illness.

196
00:30:59.520 --> 00:31:09.369
wim cuypers: so diseases that are spread through viruses through mosquitoes. For example, thinking about viruses that are spread via mosquitoes and antimicrobial resistance. Or Amr.

197
00:31:10.550 --> 00:31:34.019
wim cuypers: If you look for numbers about how many people get affected by infectious diseases, it's difficult to estimate. But you can say. For example, you can find numbers for 2019. There were 30 million deaths mainly in children and adolescents, and then you see that it's mainly enteric infections, low respiratory tract infections think about respiratory viruses, for instance, and malaria.

198
00:31:34.390 --> 00:31:44.809
wim cuypers: And one problem that I that we see is that low and middle income countries are mainly affected or disproportionately affected by this infectious disease.

199
00:31:45.080 --> 00:31:55.880
wim cuypers: and I believe that there is a necessity for detailed genomic monitoring for this infectious disease. I think this has become very apparent. Also during the COVID-19 pandemic.

200
00:31:56.290 --> 00:32:02.925
wim cuypers: there's great potential of microbial genomics. So studying the genomes of of pathogens. Basically

201
00:32:03.430 --> 00:32:29.779
wim cuypers: why? Because we can in the future replace complex laboratory techniques by informatics and sequencing. We end up with a lot of information, high resolution information. And there's a lot of potential, I would say in, for example, mobile sequencing and techniques such as adaptive sampling that can enrich your samples while sequencing. And I'll come back to that. That's an important

202
00:32:29.890 --> 00:32:45.260
wim cuypers: part of our talk. To give you an example. Why, microbial genomics is important for my Phd. One of the main outputs of my Phd. Was this study on Salmonella concourse, the Salmonella Cerrovar mainly found in Ethiopia, and we saw that

203
00:32:45.940 --> 00:33:01.889
wim cuypers: it was not clear how this strain, or how this Serovis was circulating in Ethiopia, and we found that mostly the strains from Ethiopia were very resistant. By studying all these genomes over 300 genomes. Of these bacteria.

204
00:33:01.890 --> 00:33:16.790
wim cuypers: we found that there were multiple introductions of these strains in Ethiopia, multiple introductions of antimicrobial resistance. And when I started doing Nanopart sequencing, I could see that there was a lot of structural variability in the genes encoding for antimicrobial resistance.

205
00:33:16.790 --> 00:33:27.509
wim cuypers: which is something we didn't pick up with other sequencing technologies. In addition, the Nanopore sequencing is something that is very mobile for some of the of the sequencing devices. So

206
00:33:27.540 --> 00:33:37.829
wim cuypers: and start really getting interesting in in this technology. And luckily my supervisor, Professor Chris Locus, was also very interested in this technology as a data scientist

207
00:33:37.880 --> 00:33:46.759
wim cuypers: and or as a bio bio data mining professor. So we teamed up again for my poster project, which is lipstick.

208
00:33:47.040 --> 00:33:56.509
wim cuypers: which is about making algorithms for efficient nanopar data processing. And, for example, also efficient signal classification

209
00:33:56.800 --> 00:33:58.270
wim cuypers: for Nanopore, sequencing.

210
00:33:58.820 --> 00:34:08.859
wim cuypers: The 1st study that we did here was on malaria. Actually, so, this was together with the Institute of Medicine, with the malaria unit.

211
00:34:09.139 --> 00:34:34.129
wim cuypers: together with them. We sequenced blood samples. I'll come back to that. But 1st I'll highlight again. Why, Nanopart, sequencing is an interesting tool to study this first.st It's a mobile, sequencing technology. As you can see, it interfaces with a laptop, and it generates very long reads. And so these long reads contain more information for certain research questions.

212
00:34:34.429 --> 00:34:46.110
wim cuypers: Another advantage that is very difficult to do with other sequencing technologies, real time data processing and and getting your data insights really rapidly.

213
00:34:47.150 --> 00:34:57.189
wim cuypers: If you go back to the, to the malaria problem, let's say, if we take a blood sample of a human patient that is likely infected with malaria

214
00:34:57.230 --> 00:35:19.669
wim cuypers: will have a blood sample consisting of infected red blood cells, so red blood cells infected by malaria parasites. Most of red blood cells will hopefully not be infected. And then there are also leukocytes, the white blood cells that also contain a lot of human DNA, of course. And so if you extract DNA from such a sample there will be mainly human DNA, and not a lot of

215
00:35:19.780 --> 00:35:25.730
wim cuypers: plasmodium DNA, which we preferably would want to investigate using Nanopore sequencing.

216
00:35:25.880 --> 00:35:28.720
wim cuypers: And so the solutions that we had that we have

217
00:35:29.010 --> 00:35:43.789
wim cuypers: that are mainly used are laboratory based solutions, for example, deplete the leukocytes to deplete the human DNA, or do some sort of enrichment technique, using selective or genome amplification.

218
00:35:43.990 --> 00:35:49.179
wim cuypers: or use probes to filter out. The DNA that interests you.

219
00:35:49.690 --> 00:35:56.259
wim cuypers: But we were thinking about. Let's maybe we can use this adaptive sampling technique of Nanopore, which is, which appears to be very interesting.

220
00:35:56.340 --> 00:36:02.380
wim cuypers: Adaptive sampling mainly means that you, while you are sequencing and and Nanopore sequencing is all about.

221
00:36:02.752 --> 00:36:07.590
wim cuypers: A pore, and and there's DNA going through while the DNA is going through the pore

222
00:36:08.098 --> 00:36:11.159
wim cuypers: an Ionic current is disrupted, and and we measure that signal.

223
00:36:11.350 --> 00:36:22.219
wim cuypers: And while we are sequencing we can interpret the signal. And it's actually possible, while you are sequencing, to eject the DNA that you are sequencing. If it's not on target.

224
00:36:22.390 --> 00:36:48.229
wim cuypers: if you reject it, you can sequence more of what you are interested in. So basically, let's say, I'm sequencing a blood sample of a patient. I'm doing live base calling. And I'm seeing, okay, this is plasmodium malaria. DNA. I'm going to continue to sequence it. And if it's not, if it's human, DNA, I'm going to eject it. So I have not wasted a lot of resources on sequencing this. Read, and I will never sequence it again. And I will continue sequencing the next read.

225
00:36:48.360 --> 00:36:53.432
wim cuypers: And we we were in our lab. We were really intrigued by this technique. Also because

226
00:36:53.840 --> 00:37:08.649
wim cuypers: we can, we have algorithmic ids that we can can link link up with this technology. So we were wondering, can adaptive sampling result in faster pathogen characterization from these patient samples of patients that have malaria.

227
00:37:09.820 --> 00:37:14.065
wim cuypers: So we did a benchmark experiment where we basically mixed

228
00:37:14.800 --> 00:37:32.950
wim cuypers: as pure as possible plasmodium, DNA DNA of malaria, parasite and human DNA in different concentrations, and then we split the Nanopore flow cell into parts. One part was just sequencing everything, and the other part was doing adaptive sampling where we have

229
00:37:32.950 --> 00:37:45.510
wim cuypers: let the computer decide which DNA we sequence and which DNA. We don't sequence because it's on the same flow cell, we were able to accurately determine an enrichment for each of the concentrations.

230
00:37:45.850 --> 00:37:59.580
wim cuypers: and we noticed that there's a 3 to 5 fold enrichment for the samples where there is not a lot of plasmodium DNA present. If there's a lot of human DNA present, the enrichment potential will be lower.

231
00:38:00.760 --> 00:38:09.489
wim cuypers: And then, of course, the next step would be to check whether this works in patient samples. So what we did there is sequence. Take a blood sample of human patients

232
00:38:09.600 --> 00:38:36.910
wim cuypers: extract DNA start sequencing it completely in adaptive sampling mode. And surprisingly, it worked very well. We were able to do drug resistance typing. We were able to look at structural variants. So we ended up with enough target DNA without doing any complex lab techniques without spending a lot of time in the laboratory just by having the nanopar sequence and the computer do its work on extracted DNA of a blood sample.

233
00:38:39.610 --> 00:38:47.940
wim cuypers: So we then here's an overview figure of of how much time it takes for the other other techniques that you can do in the lab.

234
00:38:48.000 --> 00:39:00.070
wim cuypers: You can see techniques here like they take 17 h, sometimes 66 h. We can skip all these techniques and just do DNA extraction library prep sequencing and have the computer

235
00:39:00.407 --> 00:39:04.009
wim cuypers: do the adaptive sampling and and reduce the sample this way.

236
00:39:04.230 --> 00:39:09.449
wim cuypers: The only disadvantage at the moment, especially if you're thinking about using it in low and middle income countries.

237
00:39:09.670 --> 00:39:15.859
wim cuypers: is mainly the the price which is prohibitive for using it in in low and middle income countries at the moment.

238
00:39:16.540 --> 00:39:31.849
wim cuypers: But another advantage is that it's free of amplification bias because we we saw that there are a lot of kinetic reads. If we use a technique like selective whole genome amplification and the depth of coverage is also a lot less stable if you do selective whole genome amplification.

239
00:39:32.550 --> 00:39:41.120
wim cuypers: But again, the price is prohibitive, and also we need to make sure that the pathogen. Abundance in the sample is high enough.

240
00:39:41.170 --> 00:39:50.080
wim cuypers: That's another finding from the study. The DNA fragment size needs to be higher. And so for some sample types which we tested it doesn't. It doesn't work.

241
00:39:50.420 --> 00:40:14.559
wim cuypers: But for the technology aspects we were thinking, like, if you can do faster squiggle classification using algorithms, new algorithms using machine learning, we could do maybe more enrichment. And also because these nanopar sequences are becoming more and more miniaturized. We need these algorithms to operate also on cpus and not only on Gpus. So we need these algorithmic solutions.

242
00:40:14.560 --> 00:40:27.879
wim cuypers: But then we had a problem because there wasn't any data to make these algorithmic solutions on the new new type of of nanopar data. So that led us to the the last project I will present, which is squid base.

243
00:40:28.120 --> 00:40:32.379
wim cuypers: A squid base is a resource of microbial squiggle data.

244
00:40:32.810 --> 00:40:33.950
wim cuypers: and

245
00:40:34.460 --> 00:40:56.700
wim cuypers: a squiggle is for Nanopore fluctuating current over time. That is how the raw signal looks like in Nanopore, sequencing for illumina. Sequencing the raw signal is actually an image. But for Nanopore sequencing, it's this current. It's stored in the fast 5 or pot 5 files. But usually people work with the base called data that you have in your Fastq file.

246
00:40:56.870 --> 00:41:04.640
wim cuypers: The fast Q file is then uploaded to Ena is array, and people forget to upload the the raw data, the actual raw data. So the the files.

247
00:41:05.830 --> 00:41:08.420
wim cuypers: And what's what is

248
00:41:08.450 --> 00:41:14.410
wim cuypers: annoying about that? Is that so? PE. Most people throw a lot of people throw away their their squiggles.

249
00:41:14.430 --> 00:41:31.130
wim cuypers: And if you do want to work with these squiggles for my informatics for data mining making algorithms. You know that you find that these are scattered across different resources. They're rarely uploaded on Sra. There's no data for the latest Nanopore chemistry.

250
00:41:31.504 --> 00:41:40.360
wim cuypers: And that is because it's bulky data. And there's not really an incentive to do so unless you start thinking more from the

251
00:41:40.910 --> 00:41:42.669
wim cuypers: from a data science perspective.

252
00:41:43.740 --> 00:41:57.011
wim cuypers: I'm I'm I'm advocating that we should keep it. So we should keep this data. A fast Q file is not our primary data or put 5 file is with with the Nanopore squiggles. And these base colors that convert this, this

253
00:41:57.430 --> 00:41:59.700
wim cuypers: transformer models that convert

254
00:41:59.740 --> 00:42:09.480
wim cuypers: the the current signal to a actual sequence, they're becoming better and better. So your skew files actually get become outdated.

255
00:42:09.792 --> 00:42:19.399
wim cuypers: Also, by making if we should keep. If we keep our data and make it available. We can make community based algorithms. Maybe there are better algorithms than the one that are commercially available.

256
00:42:19.841 --> 00:42:27.580
wim cuypers: And so we we found that there was a need for a public database, which is why we made squid base

257
00:42:28.050 --> 00:42:32.909
wim cuypers: squid base to hold this now around Nanopart data and make it findable.

258
00:42:33.180 --> 00:42:38.529
wim cuypers: As you can see, it's a very user friendly interface. Squid base.

259
00:42:38.710 --> 00:42:47.789
wim cuypers: There's a very easy upload so you can just drag and and drop your files, your old 5 files. In this case you add a Csv file with metadata

260
00:42:47.880 --> 00:42:55.710
wim cuypers: and that metadata. We made it as rich as possible, so that we can also thinking about later future applications.

261
00:42:56.134 --> 00:43:00.029
wim cuypers: So this is an example of all the all the metadata

262
00:43:00.911 --> 00:43:02.618
wim cuypers: that we that we keep.

263
00:43:03.543 --> 00:43:18.439
wim cuypers: We also provide we also will provide a pipeline to preprocess your data, because if you sequence something, usually, there's not only let's say, the target species that you are sequencing. So to filter all that out, filter out the

264
00:43:18.700 --> 00:43:28.760
wim cuypers: the interesting data to upload, to squidbase. We made this pre-processing pipeline in next flow. It also gives you coverage statistics and and depth of coverage statistics about your

265
00:43:28.790 --> 00:43:29.980
wim cuypers: samples.

266
00:43:31.830 --> 00:43:52.599
wim cuypers: Currently, what we have in squid basin, what we will make available in a couple of weeks is a lot of virus data. Because we teamed up with the Institute of Tropical Medicine. They have a lot of interesting viruses. And we sequenced all these viruses in our lab. So we are actually compute a lab in the department of computer science with an actual sequencing lab as well.

267
00:43:52.910 --> 00:44:05.179
wim cuypers: And we sequenced these viruses. We also included previous publicly available data, which is not a lot actually. And then we also included the plasmodium data of the previous study that I presented.

268
00:44:06.280 --> 00:44:15.070
wim cuypers: Looking ahead. We want to launch squid base very soon. It's already online, but the data is not downloadable yet, so that will be

269
00:44:15.806 --> 00:44:20.560
wim cuypers: coming soon. Our our database engineer, Halil, is working hard on this.

270
00:44:20.610 --> 00:44:49.119
wim cuypers: We also want to include squiggle matching and visualization. I think there's a huge potential here for people studying epigenetics. There's also an enormous potential of rapidly matching squiggles. I think that's even should be even faster than the base calling approach that we have now. And these are actual projects that are ongoing in the Atom Data lab of Chris Laukins, and that will be included in a newer version of Squidbase as well.

271
00:44:51.700 --> 00:45:00.759
wim cuypers: While I'm approaching the end of this presentation, I want to thank the project team of Squidbase, and also Halil, who has is actually developing squid, base.

272
00:45:00.810 --> 00:45:03.289
wim cuypers: and and all our

273
00:45:03.620 --> 00:45:04.830
wim cuypers: support.

274
00:45:04.930 --> 00:45:17.830
wim cuypers: and also people from the Institute of Tropical Medicine. They have given us these viruses for sequencing, and if you are interested in uploading your raw data for your grown antibod data, you can always contact me. Of course.

275
00:45:18.840 --> 00:45:31.670
wim cuypers: as a last unrelated point. I also wanted to mention that I'm the vice chair of the Icb Student Council. It's a enormous community of regional student groups spread across the world.

276
00:45:31.690 --> 00:45:39.088
wim cuypers: If you are interesting in collaborating for events, you can always contact me, and I will link you with the correct people.

277
00:45:39.800 --> 00:45:46.779
wim cuypers: so then, this is the end of my presentation. I thank you for the invitation. I'm and I'm happy to take questions.

278
00:45:53.400 --> 00:45:59.139
stein.aerts@vib.be: Thank you very much. Rin for the really interesting talk. Do we have any questions in the audience here or online on Zoom.

279
00:46:01.270 --> 00:46:03.960
stein.aerts@vib.be: I've got a couple.

280
00:46:04.180 --> 00:46:18.289
stein.aerts@vib.be: So yeah, this is really cool. I remember when Nanopore sequences 1st came out and the the big goal was being able to take these like in long distances and take them wherever they're needed to sequence these things. The the use of the kind of targeted approach is really really cool as well.

281
00:46:18.300 --> 00:46:26.670
stein.aerts@vib.be: I think that's a really nice approach you mentioned. The cost is a bit prohibitive here. Can you do any kind of sample multiplexing to bring that down on a per sample, basing.

282
00:46:27.730 --> 00:46:38.470
wim cuypers: Yeah, totally, so multiplexing is is definitely possible. We, we barcode samples like with any other sequencing technology. And then you can start multiplexing.

283
00:46:38.540 --> 00:46:57.610
wim cuypers: It's so for some, if if we look at the plasmodium, study some samples, it will be easier if there's enough parasites in the blood. Basically, then you can multiplex a lot of them. But like 5, for example, if it's low parasitemia, it's it's not possible yet. So for that, we need more better algorithm. I would say.

284
00:46:58.150 --> 00:47:16.060
stein.aerts@vib.be: Yeah, okay, that makes sense. And the other thing. Yeah, the the squiggle matching that you were showing, not something I'd ever really considered to be a possibility. But I like the idea, because, like you said, in a lot of cases, you could just avoid base calling, or at least avoid base calling on the whole signal in this case is, if you've got a known a known match, then

285
00:47:16.090 --> 00:47:28.060
stein.aerts@vib.be: my assumption is, you can say, well, we've got a good signal for a known match. Here we we don't base call on this area, but this particular fragment goes outside of that, and we only base call on on those regions is that kind of the idea you guys are having with that.

286
00:47:28.680 --> 00:47:30.589
wim cuypers: That is exactly what the idea is. Yeah.

287
00:47:31.250 --> 00:47:38.810
stein.aerts@vib.be: Yeah, that's I think that's really cool. I think that will well, hopefully improve the the speed of the data output for these kind of devices. So

288
00:47:38.820 --> 00:47:48.069
stein.aerts@vib.be: yeah, we're still running a little behind. So I'm going to leave it there. If anyone has questions, you can reach out to wim. His email address is on screen. So thank you again, Wim.

289
00:47:48.280 --> 00:47:56.810
stein.aerts@vib.be: And with that we'll move on to our 3rd speaker, which is David Novak from the lab of Ivan Seiss.

290
00:47:56.820 --> 00:47:59.849
stein.aerts@vib.be: who should just be able to take over now.

291
00:48:00.270 --> 00:48:03.270
David Novak: Yes, good morning, everybody. Am I audible?

292
00:48:03.270 --> 00:48:04.539
stein.aerts@vib.be: You are audible.

293
00:48:04.540 --> 00:48:33.919
David Novak: Good, and I assume that you can now see my presentation. So I'll just put it on full screen. All right. So yeah, thanks for giving me the opportunity to give a short talk here. I'm a Phd. Student from the lab of Yvonne Science at the Center for Inflammation research. The vib in Ghent. And so the title of my talk is interpretable models for lower dimensional embeddings of multi-scale structures in single cell data which is a mouthful. But basically.

294
00:48:33.920 --> 00:48:57.800
David Novak: the presentation touches on a few open problems in the field of dimensionality reduction and some new contributions to it that we provide in the form of a preprint that we're trying to get published now, and 2 new Python frameworks, Vive and Vscore, which I developed with colleagues from the Vib, and with some external collaborators from us.

295
00:48:57.910 --> 00:48:59.029
David Novak: So

296
00:49:00.460 --> 00:49:06.550
David Novak: let's start with the sort of fundamental motivating problem

297
00:49:06.570 --> 00:49:18.979
David Novak: modern single cell experiments can measure gene transcription, protein expression, or other biological features at a single cell resolution, which is very exciting.

298
00:49:19.210 --> 00:49:46.789
David Novak: However, the data is high, dimensional, often, and often very, very sparse. So we have a, we have tabular data with basically a lot of zeros in it. And this presents a challenge for visualizing and exploring the data as well as analyzing the data in computational computational pipelines. And because of this we often use

299
00:49:46.790 --> 00:49:51.850
David Novak: dimensionality reduction to create lower, dimensional

300
00:49:51.900 --> 00:50:00.129
David Novak: sort of salient embeddings of the data. For various purposes, however, these embeddings will inherently introduce distortion

301
00:50:00.330 --> 00:50:13.390
David Novak: and some loss of structure in comparison to the to the input data, which is again high, dimensional, and difficult to interpret. So the idea is to understand these distortions and minimize them.

302
00:50:13.861 --> 00:50:19.599
David Novak: There are a number of use cases for dimensionality production in single cell.

303
00:50:19.600 --> 00:50:44.870
David Novak: For example, two-dimensional embeddings help us to reveal artifacts such as technical batch effect. The example on the right. The picture on top is a single cell, Rna sequencing data set where you can see that a lot of these populations that are sort of color, coded

304
00:50:45.150 --> 00:51:01.609
David Novak: separate into sort of 2 separate islands, and that already already actually manages to capture a technical batch effect between different batches of samples. That is not a biological origin. So that is a small example of that.

305
00:51:01.610 --> 00:51:21.859
David Novak: These embeddings can also facilitate discovery of new cell subsets and heterogeneity, and labeled sort of labeled populations, that we might not be aware of upfront and sort of help us help us explore some some sort of higher resolution.

306
00:51:22.354 --> 00:51:26.800
David Novak: Compartmentalization of the of the different different cell populations.

307
00:51:27.100 --> 00:51:38.100
David Novak: and an interesting example is they can even reveal plausible candidates for developmental pathways, for example, in data from primary

308
00:51:38.831 --> 00:51:41.739
David Novak: immune organs, and so on.

309
00:51:42.310 --> 00:52:03.899
David Novak: So there are also important uses. For, let's say, medium dimensional embeddings. Where we go, for instance, from thousands of features to 50 or 100 and an extremely common use case. Here is the pre-processing of single cell. Rna sequencing data with principal component analysis, which is a linear reduction.

310
00:52:03.900 --> 00:52:16.200
David Novak: In addition to that, some algorithms use nonlinear embeddings to do clustering or trajectory inference, so inferring developmental pathways

311
00:52:16.260 --> 00:52:32.109
David Novak: or batch effect correction in a reduced latent space. So the idea is here. Not only do we visualize a technical artifact, but we actually work in the sort of latent space representation which is a bit less sparse to correct it.

312
00:52:32.410 --> 00:52:47.599
David Novak: So in the grand scheme of things, I distinguish between 2 main uses of dimension reduction. The 1st is for exploratory analysis, where we try to better understand structures in our data and generate some hypotheses about it.

313
00:52:47.730 --> 00:52:55.919
David Novak: And the second is to generate input for downstream analyses. So I sort of color coded them here

314
00:52:56.316 --> 00:53:09.859
David Novak: in blue and in green. And of course we can go a bit bit more into detail. Because, this is just a very coarse sort of categorization into 2 main uses for this.

315
00:53:10.790 --> 00:53:20.559
David Novak: So with regard to nonlinear embeddings, we can look at roughly 3 main categories. Firstly, we have neighbor embedding algorithms

316
00:53:20.570 --> 00:53:39.759
David Novak: which try to preserve local point neighborhoods and tend to form separate clusters of points. Second, are deep learning methods based on autoencoders and variational autoencoders which learn a smooth, latent space and don't have this locality bias, and the 3rd are

317
00:53:40.173 --> 00:53:58.779
David Novak: variants of multidimensional scaling, or Mds based on conserving pairwise distances between points or some points. Somehow, when going from high dimension to low dimension, and these tend to favor global structure, and they are actually by far the least common in single cell biology.

318
00:53:58.910 --> 00:54:19.359
David Novak: So the 2 most popular methods as of now, are Tisni and Umap, and they're both neighbor embedding methods now because they allow for nonlinearities as opposed to Pca. For example, they capture more complex patterns in the data that Pca just cannot in few dimensions.

319
00:54:20.030 --> 00:54:35.269
David Novak: but they are also non-parametric meaning. There is no functional form for the transformations that they learn, and that in turn makes the embedding space or the latent space itself meaningless. It's not easy to interpret.

320
00:54:36.073 --> 00:54:46.276
David Novak: So that's that's clearly something of a drawback. Crucially, they are mostly good at preserving local structures and thereby separating out different

321
00:54:46.750 --> 00:55:00.499
David Novak: cell populations, which is why many biologists often like them because they're used to them, and they know how to interpret them. But in contrast, global structure, which is good for outlier detection or representing trajectories.

322
00:55:00.500 --> 00:55:20.790
David Novak: is often distorted, and there are ways to boost the global structure preservation somewhat through different ways of initializing the methods. And all of this is actually discussed in our preprint. So if you're interested in more, I would recommend that you take a look.

323
00:55:22.218 --> 00:55:35.031
David Novak: Another key topic, at least for me, is how to evaluate these embeddings. Because that's quite crucial. 1st of all, the notion of local versus global which I already sort of hinted at

324
00:55:35.420 --> 00:55:54.789
David Novak: local versus global structure preservation which I will abbreviate here, as Sp on the slides has been around for long, even though the definitions are not quite settled quite importantly to us. Recent approaches to evaluating embeddings overwhelmingly either use

325
00:55:54.790 --> 00:56:06.700
David Novak: downsampled data or they evaluate the embeddings, using a clustering or classification model which ends up giving a proxy score that is not at all guaranteed to be fair.

326
00:56:06.790 --> 00:56:24.730
David Novak: And an example of that would be this, we cluster on the high dimensional data, the input data, and then we cluster on the on the low dimensional representation and see if the clustering sort of matches up, or we train a classifier on a bunch of different

327
00:56:24.730 --> 00:56:48.160
David Novak: low dimensional embeddings of the same data set, and see if it does well at predicting labeled populations. So in this supervised scenario, we actually also rely on an accurate labeling of the cell populations, and we assume that the labeling really is the ground truth, and that the

328
00:56:48.280 --> 00:57:01.999
David Novak: the labeling is done at a stable resolution across all different cell compartments represented in the data. So there are a bunch of assumptions that might or might not hold.

329
00:57:02.290 --> 00:57:24.730
David Novak: and that is a pity. In contrast, there is a nice theoretically sound approach. That is a graph-based method called Rnx curves. And this method scores structure preservation by computing how neighborhood ranks, meaning relative positions of points with respect to each other. In the entire point, cloud

330
00:57:24.730 --> 00:57:41.520
David Novak: are preserved as we go from high dimension to low dimension. The great thing about Rnx curves is that they allow us to evaluate structure objectively and at multiple scales going from local to to sort of global. But the

331
00:57:41.740 --> 00:57:56.300
David Novak: the drawback is that we cannot use them for large data sets because the algorithm scales very poorly with number of input input points. Actually, it scales quadratically because we need to basically calculate the entire distance matrix, which is a problem.

332
00:57:56.760 --> 00:58:06.149
David Novak: So having gone over the existing challenges, I'll reiterate the main problems, we decided to tackle and how we did it. So the 1st problem

333
00:58:06.310 --> 00:58:09.760
David Novak: is that there is limited interpretability of

334
00:58:09.910 --> 00:58:32.139
David Novak: dimension reduction models. Remember that I said this particularly in the examples of Tisney and Umap. There is actually basically no interpretability of the latent space, and there is also limited. There are limits to the structure preservation achieved by these methods. And so to address this, we created a new model called viva, based on variational autoencoders, but with some improvements

335
00:58:32.310 --> 00:58:50.869
David Novak: so specifically vive, improves local structures by applying a denoising step to the input, data and it improves the global structure by using a novel loss function called stochastic Mds based on the multidimensional scaling paradigm, and to give

336
00:58:51.090 --> 00:58:55.908
David Novak: a bit of a an idea of what the stochastic Mds does.

337
00:58:56.570 --> 00:59:08.429
David Novak: let's say the model is given 4 points from very distant parts of the input space. And unfortunately, I cannot plot a hundred dimensional input space. So I will have to resort to this sort of 3 dimensional mock up.

338
00:59:08.470 --> 00:59:13.640
David Novak: It learns to output an embedding low dimensional embedding for each point.

339
00:59:13.980 --> 00:59:33.909
David Novak: and then it uses the relative distances within that quartet of points or group of 4 points to jointly optimize the geometry of the embedding, so that their sort of relative positions are are more correct, and this is done for many groups of points throughout the training repeatedly and that way the embedding

340
00:59:34.060 --> 00:59:55.469
David Novak: the embedding function has a regularizer that makes sure that the global structure is actually is actually reflected better. I know this is a lot of arrows, but if you want to actually read through the pseudocode and the equations, please take a look at our paper, and you'll get a proper understanding of what we did. So

341
00:59:56.040 --> 01:00:21.690
David Novak: our model also has an integrated tool drawing from differential geometry that detects local distortion of its learned latent space or embedding space that we call encoder indicatrices. And in practice this means that we can detect local sort of expansion or shrinking of some parts of the low dimensional space. So this would be a two-dimensional point cloud representing a high dimensional data set.

342
01:00:21.920 --> 01:00:48.300
David Novak: and you can see that in this case there is not that much distortion, but there is. If you look at these circular or elliptical shapes sometimes there's a bit of difference in size. And also we can detect directed stretching because the shape also changes. And this is actually a visualization of of distortions that happen locally within different parts of the

343
01:00:48.300 --> 01:01:07.109
David Novak: of the latent space, and we can sort of visualize them this way. And it turns out that with a vanilla variational autoencoder, you get a lot of distortion, and we actually reduce these distortions somewhat, and I will show, I think, one more figure for this

344
01:01:07.210 --> 01:01:10.088
David Novak: in like 2, 2 or 3 slides.

345
01:01:10.600 --> 01:01:35.349
David Novak: So the second problem we decided to confront is, in our view, insufficient evaluation, and we did that with a framework called vscore. The main contribution here is a highly accurate approximation algorithm for the Rnx curves which lets us use them with large biological data sets. So we have a sort of unbiased evaluation metric.

346
01:01:35.430 --> 01:01:40.820
David Novak: And we can also evaluate local and global structure preservation separately.

347
01:01:41.050 --> 01:01:50.269
David Novak: We also provide a supervised scoring method that I will not describe in in detail here in the interest of time, and

348
01:01:50.530 --> 01:02:03.070
David Novak: we also have an automated benchmarking and hyperparameter tuning framework that you can run on the Hpc. If you're interested in how accurate your embeddings of high dimensional data sets are.

349
01:02:04.520 --> 01:02:14.340
David Novak: So I will just show a few bits of results from our paper hopefully to pique your interest, but we'll keep it short, because I can see that we're already over time. So

350
01:02:14.750 --> 01:02:34.870
David Novak: first, st our benchmarking setup allowed us to compare local and global structure preservation by different methods across different data sets. And here we show results for 8 single cell Rna sequencing data sets with local structure on the X-axis and global structure on the Y-axis, and each of the subplots corresponds to a different data set.

351
01:02:35.360 --> 01:02:51.570
David Novak: And there are a few notable things. For example, Tisney, which is, the blue square usually excels at local structures. So it's all the way on the right. In each case, however, it is also quite.

352
01:02:51.640 --> 01:02:54.879
David Novak: quite low in global structure preservation

353
01:02:55.232 --> 01:03:02.879
David Novak: and other methods strike a different balance. And we actually show the Pareto front for each data set here using the dashed line.

354
01:03:03.420 --> 01:03:26.609
David Novak: And for our method. We tested 2 variants and they end up here. So I should stress that since there is a trade-off between the local and global, there is no such thing as the universally best method, but it's certainly valuable to be able to compare them in a systematic way here across different data sets.

355
01:03:27.770 --> 01:03:39.029
David Novak: So we also have case studies with specific data. This is an example of an embryological data set from zebrafish collected at different stages of development.

356
01:03:39.920 --> 01:03:45.600
David Novak: And it is interesting because the data has both discrete time points. So it's longitudinal data

357
01:03:45.840 --> 01:04:00.899
David Novak: where we want cell subsets to be clearly separated as well as some developmental gradients where we want the transitions to remain smooth. So we actually have both of these, both of these sort of

358
01:04:01.020 --> 01:04:03.930
David Novak: opposing criteria.

359
01:04:04.300 --> 01:04:15.779
David Novak: So we actually know this because we have access to annotated trajectories from the authors of this data where they check the trajectories against known expression expression patterns.

360
01:04:16.230 --> 01:04:39.099
David Novak: and we can quantify sort of the gaps between the different points, as we move from the stem cells, from the progenitors to all the more differentiated cells. So on the left we see that Tisni and Umap tend to break the data apart into islands, whereas escort Mds. Which is a recent Mds based method

361
01:04:39.624 --> 01:04:46.970
David Novak: and a vanilla autoencoder and vive our method, create smoother embeddings of the data.

362
01:04:47.140 --> 01:05:00.239
David Novak: We show that Tisni and Umap, in fact, break some trajectories apart which is undesirable. The Mds method, on the other hand, tends to smoothen everything out too much, and does not resolve distinct pathways.

363
01:05:00.400 --> 01:05:22.669
David Novak: and the Vae-based methods strike a favorable balance. Additionally, if we look at some annotated pathways, we see that the gradient from the progenitor in blue here toward the most differentiated cells in yellow, and then the very, very last percentile of cells in red. Here

364
01:05:22.670 --> 01:05:50.860
David Novak: the gradient is more apparent in the smoother embeddings, and in the paper we look at more pathways, and crucially we make some quantitative comparisons also, so that we do not rely only on impressions. But this is just to show the fundamental sort of difference between the different embeddings. And finally, I should wrap up. I want to mention that we show that the stochastic Mds loss, which is the regularizer that we have

365
01:05:51.472 --> 01:06:13.149
David Novak: which improves. Global structure also limits distortions of the latent space that we commonly observe with autoencoders and variational autoencoders. So this is maybe more of theoretical interest. But I think it's pretty cool. We show this using the Encoder indicatrices, which is this way of revealing the distortions.

366
01:06:13.680 --> 01:06:31.809
David Novak: And the key thing here is that both these Encoder indicatrices and the stochastic Mds regularizer can easily be adopted for use with any model that has an encoder doesn't even have to have the full autoencoder. It only relies on a differential encoder network.

367
01:06:32.040 --> 01:06:47.029
David Novak: And so the idea is that we can effectively detect and reduce distortion in other models, not just the specific one that we propose, and we hope that we can sort of advance the course of more interpretable

368
01:06:47.130 --> 01:07:07.410
David Novak: and less distorting models for structure, learning, and low dimensional embeddings of single cell data. So this concludes my somewhat hasty overview. I would like to thank my my supervisors and collaborators, both from our lab and from, and the Vive and vscore packages are available at the link.

369
01:07:07.410 --> 01:07:17.210
David Novak: On the right we include a number of tutorials and case studies, and we also include a link to the preprint. And yeah, that's it. And thank you very much for your attention.

370
01:07:22.060 --> 01:07:26.030
stein.aerts@vib.be: Thank you very much, David. You are right about time, but if we have a quick question.

371
01:07:26.160 --> 01:07:27.050
stein.aerts@vib.be: then

372
01:07:27.330 --> 01:07:28.689
stein.aerts@vib.be: we will go with that. Yeah.

373
01:07:30.575 --> 01:07:48.350
stein.aerts@vib.be: thank you for the great work. I wonder like regarding like posterior collapse. I'm not sure if you ever faced this with this type of implementation on the variation Autoencoder. Like, I'm curious about how this regalizer work with like on the encoder with the posterior collapse. If you ever faced it.

374
01:07:50.940 --> 01:07:57.989
David Novak: not really. I can't. Yeah, I don't know if I can really comment on that, because we haven't really.

375
01:07:58.690 --> 01:08:01.650
David Novak: Yeah, I don't know. I don't. I don't really know if I can

376
01:08:02.220 --> 01:08:04.620
David Novak: give you any insight on that.

377
01:08:05.160 --> 01:08:06.080
stein.aerts@vib.be: Thank you as well.

378
01:08:06.080 --> 01:08:07.850
David Novak: Apologies, though I'm sorry.

379
01:08:09.445 --> 01:08:19.659
stein.aerts@vib.be: If we've got nothing else. I got one very quick one. I'm just curious what the kind of computational time of V-vave is compared to Tsneve on Umap, that kind of thing? Are we talking same order of magnitude or.

380
01:08:21.050 --> 01:08:45.240
David Novak: It depends on. It depends on whether you can train on Gpu, of course, but it's I mean it scales linearly with the size of the data set, and it will, it will be some. In practice. It will be somewhere in between Umap and Tisni, for transcriptomic data sets. If you do the best reduction with Pca to 100 principal components so similar, you know not a difference in order of magnitude.

381
01:08:45.240 --> 01:08:51.839
David Novak: but of course it depends on the number of epochs that you that you train for, but with a reasonable early stopping criterion

382
01:08:51.840 --> 01:08:53.419
David Novak: somewhere in between the 2.

383
01:08:53.710 --> 01:09:02.559
stein.aerts@vib.be: Okay, excellent thanks. With that I will thank again all 3 of our speakers. Thank you. Everybody that's here in person and online for joining us.

384
01:09:02.609 --> 01:09:05.220
stein.aerts@vib.be: and we hope to see you next month.

